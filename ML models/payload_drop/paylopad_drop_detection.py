# -*- coding: utf-8 -*-
"""paylopad_drop_detection.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1CV-v5JiB5jann9X5S_1O6PszR_RFMhKD
"""

!pip install -q ultralytics roboflow

from roboflow import Roboflow
rf = Roboflow(api_key="u9cECza5gdXl0fOGIlaN")
project = rf.workspace("aerothonteam").project("payload_drop_detection")
dataset = project.version(2).download("yolov8")

from ultralytics import YOLO
model = YOLO('yolov8n.yaml')

model.train(
    data='/content/Payload_drop_detection-2/data.yaml',
    epochs=30,
    imgsz=416,
    batch=32,
    device=0  # Set to 'cpu' if GPU not available
)

model = YOLO('runs/detect/train/weights/best.pt')

# Export to TFLite format
model.export(format='tflite')

from google.colab import files
files.download('runs/detect/train/weights/best_saved_model/best_float32.tflite')

!pip install tflite-runtime opencv-python numpy
!pip install --upgrade numpy

!pip install numpy==1.24.3

import cv2
import numpy as np
import tflite_runtime.interpreter as tflite
import os

# === CONFIG ===
model_path = 'runs/detect/train/weights/best_saved_model/best_float32.tflite'
video_path = '/content/AdobeStock_406113599_Video_HD_Preview.mov'  # replace with your actual video
output_path = 'annotated_output.mp4'
input_size = 416
class_name = 'target'
conf_threshold = 0.3

# === LOAD TFLITE MODEL ===
interpreter = tflite.Interpreter(model_path=model_path)
interpreter.allocate_tensors()
input_details = interpreter.get_input_details()
output_details = interpreter.get_output_details()

# === VIDEO CAPTURE SETUP ===
cap = cv2.VideoCapture(video_path)
frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
fps = int(cap.get(cv2.CAP_PROP_FPS))
out = cv2.VideoWriter(output_path, cv2.VideoWriter_fourcc(*'mp4v'), fps, (frame_width, frame_height))

print("[INFO] Starting inference...")

while cap.isOpened():
    ret, frame = cap.read()
    if not ret:
        break

    # Preprocess frame
    input_img = cv2.resize(frame, (input_size, input_size))
    input_data = np.expand_dims(input_img, axis=0).astype(np.float32) / 255.0

    interpreter.set_tensor(input_details[0]['index'], input_data)
    interpreter.invoke()
    output = interpreter.get_tensor(output_details[0]['index'])  # shape: (1, N, 6) or similar

    detections = output[0]  # remove batch dimension

    for det in detections:
        # Handle variable output length
        if det[4] < conf_threshold:
            continue

        x_center, y_center, width, height, conf, class_conf = det[:6]
        score = conf * class_conf

        # Convert normalized center x/y/w/h to pixel x1, y1, x2, y2
        x_center *= frame_width
        y_center *= frame_height
        width *= frame_width
        height *= frame_height

        x1 = int(x_center - width / 2)
        y1 = int(y_center - height / 2)
        x2 = int(x_center + width / 2)
        y2 = int(y_center + height / 2)

        # Draw box and label
        label = f"{class_name.upper()} ({score:.2f})"
        cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)
        cv2.putText(frame, label, (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX,
                    0.5, (0, 0, 255), 2)

    out.write(frame)
    cv2.imshow("Detection", frame)
    if cv2.waitKey(1) & 0xFF == ord('q'):
        break

cap.release()
out.release()
cv2.destroyAllWindows()
print(f"[DONE] Output saved to: {os.path.abspath(output_path)}")

import cv2
import numpy as np
import tflite_runtime.interpreter as tflite
import os
# Import the cv2_imshow function from google.colab.patches
from google.colab.patches import cv2_imshow


# === CONFIG ===
model_path = 'runs/detect/train/weights/best_saved_model/best_float32.tflite'
video_path = '/content/AdobeStock_406113599_Video_HD_Preview.mov'  # replace with your actual video
output_path = 'annotated_output.mp4'
input_size = 416
class_name = 'target'
conf_threshold = 0.3

# === LOAD TFLITE MODEL ===
interpreter = tflite.Interpreter(model_path=model_path)
interpreter.allocate_tensors()
input_details = interpreter.get_input_details()
output_details = interpreter.get_output_details()

# === VIDEO CAPTURE SETUP ===
cap = cv2.VideoCapture(video_path)
frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
fps = int(cap.get(cv2.CAP_PROP_FPS))
out = cv2.VideoWriter(output_path, cv2.VideoWriter_fourcc(*'mp4v'), fps, (frame_width, frame_height))

print("[INFO] Starting inference...")

while cap.isOpened():
    ret, frame = cap.read()
    if not ret:
        break

    # Preprocess frame
    input_img = cv2.resize(frame, (input_size, input_size))
    input_data = np.expand_dims(input_img, axis=0).astype(np.float32) / 255.0

    interpreter.set_tensor(input_details[0]['index'], input_data)
    interpreter.invoke()
    output = interpreter.get_tensor(output_details[0]['index'])  # shape: (1, N, 6) or similar

    detections = output[0]  # remove batch dimension

    for det in detections:
        # Handle variable output length
        if det[4] < conf_threshold:
            continue

        x_center, y_center, width, height, conf, class_conf = det[:6]
        score = conf * class_conf

        # Convert normalized center x/y/w/h to pixel x1, y1, x2, y2
        x_center *= frame_width
        y_center *= frame_height
        width *= frame_width
        height *= frame_height

        x1 = int(x_center - width / 2)
        y1 = int(y_center - height / 2)
        x2 = int(x_center + width / 2)
        y2 = int(y_center + height / 2)

        # Draw box and label
        label = f"{class_name.upper()} ({score:.2f})"
        cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)
        cv2.putText(frame, label, (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX,
                    0.5, (0, 0, 255), 2)

    out.write(frame)
    # Replace cv2.imshow with cv2_imshow
    cv2_imshow(frame)
    if cv2.waitKey(1) & 0xFF == ord('q'):
        break

cap.release()
out.release()
cv2.destroyAllWindows()
print(f"[DONE] Output saved to: {os.path.abspath(output_path)}")